# Module 4: Capstone Project - Multi-Format Model Deployment

**Final Project: Deploying Qwen2.5-Coder Across GGUF, APR, and SafeTensors**

---

## Project Overview

In this capstone project, you will deploy the Qwen2.5-Coder-0.5B model using all three formats covered in the course: GGUF, SafeTensors, and APR. You will convert between formats, benchmark performance, and deploy to multiple targets including CLI, REST API, and WebAssembly.

| Attribute | Value |
|-----------|-------|
| **Duration** | 4-6 hours |
| **Prerequisites** | Modules 1-3 completed |
| **Model** | Qwen2.5-Coder-0.5B-Instruct |
| **Formats** | GGUF, SafeTensors, APR |
| **Deliverables** | Working deployments + benchmark report |

---

## Learning Objectives

By completing this capstone, you will demonstrate mastery of:

1. **Format Conversion** - Use Rosetta Stone to convert between all three formats
2. **CLI Deployment** - Run inference via `apr run` and `apr chat`
3. **Server Deployment** - Deploy REST API with `apr serve`
4. **Performance Analysis** - Benchmark and compare format performance
5. **Production Readiness** - Validate models with `apr check` and `apr validate`

---

## Project Structure

```
capstone/
├── models/
│   ├── qwen2.5-coder-0.5b.gguf        # Original GGUF from HuggingFace
│   ├── qwen2.5-coder-0.5b.safetensors # Converted SafeTensors
│   └── qwen2.5-coder-0.5b.apr         # Native APR format
├── benchmarks/
│   ├── gguf-benchmark.json
│   ├── safetensors-benchmark.json
│   └── apr-benchmark.json
├── deployments/
│   ├── cli/                           # CLI scripts
│   ├── server/                        # REST API configuration
│   └── wasm/                          # WebAssembly build
└── report.md                          # Final benchmark report
```

---

## Part 1: Model Acquisition and Conversion

### 1.1 Download the Base Model

Start by importing the Qwen2.5-Coder-0.5B model from HuggingFace:

```bash
# Create project directory
mkdir -p capstone/models
cd capstone

# Import GGUF format (quantized, CPU-optimized)
apr import hf://Qwen/Qwen2.5-0.5B-Instruct-GGUF \
    --quantization q4_k_m \
    -o models/qwen2.5-coder-0.5b.gguf

# Verify the download
apr inspect models/qwen2.5-coder-0.5b.gguf
```

**Expected Output:**
```
Model: Qwen2.5-0.5B-Instruct
Format: GGUF v3
Quantization: Q4_K_M
Parameters: 494M
Vocabulary: 151,936 tokens
Architecture: qwen2
Layers: 24
Hidden Size: 896
Attention Heads: 14
```

### 1.2 Convert to SafeTensors

Use the Rosetta module to convert GGUF to SafeTensors:

```bash
# Convert GGUF → SafeTensors
apr rosetta convert \
    models/qwen2.5-coder-0.5b.gguf \
    models/qwen2.5-coder-0.5b.safetensors \
    --verify

# Inspect the converted model
apr inspect models/qwen2.5-coder-0.5b.safetensors
```

**Verification Checklist:**
- [ ] Tensor count matches original
- [ ] Metadata preserved (vocab size, architecture)
- [ ] File size within expected range (dequantized will be larger)

### 1.3 Convert to APR Format

Convert to the native APR format:

```bash
# Convert SafeTensors → APR
apr rosetta convert \
    models/qwen2.5-coder-0.5b.safetensors \
    models/qwen2.5-coder-0.5b.apr \
    --compression lz4 \
    --verify

# Alternative: Direct GGUF → APR
apr rosetta convert \
    models/qwen2.5-coder-0.5b.gguf \
    models/qwen2.5-coder-0.5b-direct.apr \
    --verify

# Inspect APR model
apr inspect models/qwen2.5-coder-0.5b.apr --json
```

### 1.4 Validate All Formats

Run the 10-stage validation on each format:

```bash
# Validate each format
apr check models/qwen2.5-coder-0.5b.gguf
apr check models/qwen2.5-coder-0.5b.safetensors
apr check models/qwen2.5-coder-0.5b.apr
```

**Validation Stages:**
1. Magic bytes verification
2. Header integrity
3. Metadata completeness
4. Tensor index consistency
5. Data alignment (64-byte for APR)
6. Checksum validation
7. Tokenizer presence
8. Configuration completeness
9. Architecture compatibility
10. Inference smoke test

---

## Part 2: CLI Deployment

### 2.1 Single-Shot Inference

Test each format with the same prompt:

```bash
# Test prompt
PROMPT="Write a Python function to calculate factorial:"

# GGUF inference
apr run models/qwen2.5-coder-0.5b.gguf \
    -p "$PROMPT" \
    --max-tokens 100 \
    --temperature 0.7

# SafeTensors inference
apr run models/qwen2.5-coder-0.5b.safetensors \
    -p "$PROMPT" \
    --max-tokens 100 \
    --temperature 0.7

# APR inference
apr run models/qwen2.5-coder-0.5b.apr \
    -p "$PROMPT" \
    --max-tokens 100 \
    --temperature 0.7
```

**Record the outputs and compare:**
- Output quality (correctness of generated code)
- Token count generated
- Any differences in formatting

### 2.2 Interactive Chat

Launch interactive chat sessions:

```bash
# GGUF chat
apr chat models/qwen2.5-coder-0.5b.gguf \
    --temperature 0.7 \
    --top-p 0.9 \
    --system "You are a helpful coding assistant."

# SafeTensors chat
apr chat models/qwen2.5-coder-0.5b.safetensors \
    --temperature 0.7 \
    --top-p 0.9

# APR chat
apr chat models/qwen2.5-coder-0.5b.apr \
    --temperature 0.7 \
    --top-p 0.9
```

**Chat Test Cases:**
1. "Explain the difference between `==` and `is` in Python"
2. "Write a Rust function to reverse a string"
3. "Debug this code: `for i in range(10): print(i`"

### 2.3 Batch Processing

Create a batch inference script:

```bash
# Create test prompts file
cat > deployments/cli/prompts.txt << 'EOF'
What is 2 + 2?
Write a hello world in Rust.
Explain async/await in JavaScript.
EOF

# Batch inference (GGUF)
while IFS= read -r prompt; do
    echo "=== Prompt: $prompt ==="
    apr run models/qwen2.5-coder-0.5b.gguf \
        -p "$prompt" \
        --max-tokens 50 \
        --quiet
done < deployments/cli/prompts.txt
```

---

## Part 3: Server Deployment

### 3.1 REST API Server

Deploy each format as a REST API:

```bash
# Start GGUF server (port 8001)
apr serve models/qwen2.5-coder-0.5b.gguf \
    --port 8001 \
    --host 0.0.0.0 &

# Start SafeTensors server (port 8002)
apr serve models/qwen2.5-coder-0.5b.safetensors \
    --port 8002 \
    --host 0.0.0.0 &

# Start APR server (port 8003)
apr serve models/qwen2.5-coder-0.5b.apr \
    --port 8003 \
    --host 0.0.0.0 &
```

### 3.2 API Testing

Test the OpenAI-compatible endpoints:

```bash
# Test GGUF endpoint
curl -X POST http://localhost:8001/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "prompt": "def fibonacci(n):",
        "max_tokens": 100,
        "temperature": 0.7
    }'

# Test SafeTensors endpoint
curl -X POST http://localhost:8002/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "prompt": "def fibonacci(n):",
        "max_tokens": 100,
        "temperature": 0.7
    }'

# Test APR endpoint
curl -X POST http://localhost:8003/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "prompt": "def fibonacci(n):",
        "max_tokens": 100,
        "temperature": 0.7
    }'
```

### 3.3 Health Checks

Verify server health:

```bash
# Health endpoints
curl http://localhost:8001/health
curl http://localhost:8002/health
curl http://localhost:8003/health

# Model info endpoints
curl http://localhost:8001/v1/models
curl http://localhost:8002/v1/models
curl http://localhost:8003/v1/models
```

---

## Part 4: Performance Benchmarking

### 4.1 Throughput Benchmark

Measure tokens per second for each format:

```bash
# GGUF benchmark
apr showcase models/qwen2.5-coder-0.5b.gguf \
    --iterations 10 \
    --warmup 2 \
    --output benchmarks/gguf-benchmark.json

# SafeTensors benchmark
apr showcase models/qwen2.5-coder-0.5b.safetensors \
    --iterations 10 \
    --warmup 2 \
    --output benchmarks/safetensors-benchmark.json

# APR benchmark
apr showcase models/qwen2.5-coder-0.5b.apr \
    --iterations 10 \
    --warmup 2 \
    --output benchmarks/apr-benchmark.json
```

### 4.2 Latency Profiling

Profile P50, P95, and P99 latencies:

```bash
# Profile each format
apr profile models/qwen2.5-coder-0.5b.gguf \
    --prompt "Hello" \
    --max-tokens 50 \
    --iterations 100

apr profile models/qwen2.5-coder-0.5b.safetensors \
    --prompt "Hello" \
    --max-tokens 50 \
    --iterations 100

apr profile models/qwen2.5-coder-0.5b.apr \
    --prompt "Hello" \
    --max-tokens 50 \
    --iterations 100
```

### 4.3 Memory Analysis

Measure memory footprint:

```bash
# Memory profiling with trace
apr trace models/qwen2.5-coder-0.5b.gguf \
    -p "Test" \
    --trace-output benchmarks/gguf-trace.json \
    --memory-profile

apr trace models/qwen2.5-coder-0.5b.safetensors \
    -p "Test" \
    --trace-output benchmarks/safetensors-trace.json \
    --memory-profile

apr trace models/qwen2.5-coder-0.5b.apr \
    -p "Test" \
    --trace-output benchmarks/apr-trace.json \
    --memory-profile
```

### 4.4 Benchmark Summary Table

Create your benchmark report with this template:

| Metric | GGUF (Q4_K_M) | SafeTensors (F32) | APR (LZ4) |
|--------|---------------|-------------------|-----------|
| **File Size** | ~300 MB | ~1.0 GB | ~350 MB |
| **Load Time** | ___ ms | ___ ms | ___ ms |
| **Throughput** | ___ tok/s | ___ tok/s | ___ tok/s |
| **P50 Latency** | ___ ms | ___ ms | ___ ms |
| **P99 Latency** | ___ ms | ___ ms | ___ ms |
| **Peak Memory** | ___ MB | ___ MB | ___ MB |
| **First Token** | ___ ms | ___ ms | ___ ms |

---

## Part 5: Format Comparison Analysis

### 5.1 Output Equivalence Testing

Verify outputs match across formats:

```bash
# Generate with fixed seed for reproducibility
SEED=42
PROMPT="What is the capital of France?"

# GGUF output
apr run models/qwen2.5-coder-0.5b.gguf \
    -p "$PROMPT" \
    --seed $SEED \
    --max-tokens 20 > /tmp/gguf-output.txt

# SafeTensors output
apr run models/qwen2.5-coder-0.5b.safetensors \
    -p "$PROMPT" \
    --seed $SEED \
    --max-tokens 20 > /tmp/safetensors-output.txt

# APR output
apr run models/qwen2.5-coder-0.5b.apr \
    -p "$PROMPT" \
    --seed $SEED \
    --max-tokens 20 > /tmp/apr-output.txt

# Compare outputs
diff /tmp/gguf-output.txt /tmp/safetensors-output.txt
diff /tmp/safetensors-output.txt /tmp/apr-output.txt
```

### 5.2 Argmax Token Verification

Verify first token predictions match:

```bash
# Get argmax token for each format
apr trace models/qwen2.5-coder-0.5b.gguf \
    -p "The answer is" \
    --trace-output /tmp/gguf-trace.json

apr trace models/qwen2.5-coder-0.5b.safetensors \
    -p "The answer is" \
    --trace-output /tmp/safetensors-trace.json

apr trace models/qwen2.5-coder-0.5b.apr \
    -p "The answer is" \
    --trace-output /tmp/apr-trace.json

# Extract and compare argmax tokens
jq '.first_token.argmax' /tmp/gguf-trace.json
jq '.first_token.argmax' /tmp/safetensors-trace.json
jq '.first_token.argmax' /tmp/apr-trace.json
```

---

## Part 6: Deployment Scenarios

### 6.1 Scenario A: Edge Device (GGUF)

GGUF is optimal for edge deployment due to quantization:

```bash
# Optimal for: Raspberry Pi, consumer laptops, offline use
# Key advantages: Small file size, CPU-optimized, no GPU required

apr run models/qwen2.5-coder-0.5b.gguf \
    -p "Summarize this in one sentence: Machine learning is..." \
    --max-tokens 30 \
    --threads 4  # Limit threads for edge device
```

**Use Case:** Offline coding assistant on laptop

### 6.2 Scenario B: Cloud GPU (SafeTensors)

SafeTensors is optimal for GPU inference:

```bash
# Optimal for: Cloud instances with GPU, high-throughput servers
# Key advantages: Zero-copy loading, full precision, GPU-optimized

apr run models/qwen2.5-coder-0.5b.safetensors \
    -p "Generate a REST API in Python:" \
    --max-tokens 200 \
    --gpu  # Enable GPU acceleration
```

**Use Case:** Production inference API with GPU

### 6.3 Scenario C: Browser/WASM (APR)

APR is designed for WebAssembly deployment:

```bash
# Build WASM target
cd deployments/wasm

# Create WASM-optimized APR
apr rosetta convert \
    ../models/qwen2.5-coder-0.5b.apr \
    qwen2.5-coder-0.5b-wasm.apr \
    --target wasm \
    --optimize-size

# Build presentar UI
cargo build --target wasm32-unknown-unknown --release
wasm-bindgen target/wasm32-unknown-unknown/release/*.wasm --out-dir pkg
```

**Use Case:** In-browser code completion

---

## Part 7: Final Deliverables

### 7.1 Benchmark Report

Create `report.md` with:

1. **Executive Summary** - Key findings in 2-3 sentences
2. **Methodology** - Hardware specs, test conditions
3. **Results Table** - Complete benchmark metrics
4. **Format Recommendations** - When to use each format
5. **Lessons Learned** - Challenges encountered

### 7.2 Deployment Artifacts

Ensure your `capstone/` directory contains:

```
capstone/
├── models/
│   ├── qwen2.5-coder-0.5b.gguf         ✓
│   ├── qwen2.5-coder-0.5b.safetensors  ✓
│   └── qwen2.5-coder-0.5b.apr          ✓
├── benchmarks/
│   ├── gguf-benchmark.json             ✓
│   ├── safetensors-benchmark.json      ✓
│   └── apr-benchmark.json              ✓
├── deployments/
│   ├── cli/prompts.txt                 ✓
│   └── server/config.toml              ✓
└── report.md                           ✓
```

### 7.3 Validation Checklist

Complete all items before submission:

**Format Conversion:**
- [ ] Successfully converted GGUF → SafeTensors
- [ ] Successfully converted SafeTensors → APR
- [ ] All three formats pass `apr check`

**CLI Deployment:**
- [ ] Single-shot inference works for all formats
- [ ] Interactive chat works for all formats
- [ ] Batch processing script functional

**Server Deployment:**
- [ ] REST API responds on all three ports
- [ ] Health checks pass
- [ ] OpenAI-compatible endpoint tested

**Benchmarking:**
- [ ] Throughput measured for all formats
- [ ] Latency profiled (P50, P95, P99)
- [ ] Memory usage recorded
- [ ] Results documented in report.md

**Analysis:**
- [ ] Output equivalence verified
- [ ] Format trade-offs documented
- [ ] Deployment recommendations provided

---

## Grading Rubric

| Component | Points | Criteria |
|-----------|--------|----------|
| **Format Conversion** | 20 | All three formats created and validated |
| **CLI Deployment** | 20 | Inference works correctly for all formats |
| **Server Deployment** | 20 | REST APIs functional and tested |
| **Benchmarking** | 25 | Complete metrics with analysis |
| **Report Quality** | 15 | Clear writing, actionable recommendations |
| **Total** | 100 | Pass threshold: 80 points |

---

## Troubleshooting

### Common Issues

**Issue:** `apr import` fails with "model not found"
```bash
# Solution: Use full HuggingFace path
apr import hf://Qwen/Qwen2.5-0.5B-Instruct-GGUF
```

**Issue:** Conversion produces larger file than expected
```bash
# Expected: GGUF (quantized) < APR < SafeTensors (F32)
# Solution: Use compression for APR
apr rosetta convert input.safetensors output.apr --compression zstd
```

**Issue:** Output differs between formats
```bash
# Check: Quantization affects precision
# GGUF Q4_K_M has lower precision than SafeTensors F32
# Small differences are expected
```

**Issue:** Server fails to start
```bash
# Check port availability
lsof -i :8001
# Kill existing process if needed
kill -9 $(lsof -t -i :8001)
```

---

## Resources

### Documentation
- [APR Format Specification](https://github.com/paiml/aprender/blob/main/docs/specifications/APR-SPEC.md)
- [Rosetta Module Guide](https://github.com/paiml/aprender/blob/main/docs/rosetta.md)
- [apr-cli Reference](https://github.com/paiml/aprender/blob/main/crates/apr-cli/README.md)

### Related Course Materials
- [Lesson 1.2: Model Types & APR Format](./1.2-model-types-apr-format.md)
- [Lesson 1.3: Format Comparison & Tracing](./1.3-format-comparison-tracing.md)
- [Lesson 3.3: APR Ecosystem](./3.3-apr-ecosystem.md)

### External References
- [GGUF Specification](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
- [SafeTensors Documentation](https://huggingface.co/docs/safetensors)
- [Qwen2.5-Coder Technical Report](https://arxiv.org/abs/2409.12186)

---

## Submission

Submit your completed `capstone/` directory as a zip file or git repository link. Ensure all models are either included or downloadable via provided scripts.

**Deadline:** End of Week 4

**Good luck! Ship it!**
