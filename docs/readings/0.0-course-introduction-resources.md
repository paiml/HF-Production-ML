# Introduction to Course and Course Resources

**Course 5 of 5: Production ML with Hugging Face**

Welcome to the final course in the Hugging Face Specialization. This course focuses on deploying machine learning models to production using a pure Rust stack—no Python runtime required.

---

## Course Overview

| Attribute | Value |
|-----------|-------|
| **Duration** | 3 weeks + capstone (~14-16 hours) |
| **Prerequisites** | Courses 1-4 of HF Specialization |
| **Language** | Rust (Edition 2021, MSRV 1.83.0) |
| **License** | Apache-2.0 |
| **Modules** | 4 (3 weekly + 1 capstone) |

### What You'll Learn

By the end of this course, you will be able to:

- Deploy ML models using the Sovereign AI Stack (trueno, aprender, realizar, batuta)
- Convert between model formats: GGUF, SafeTensors, and APR
- Implement CI/CD pipelines and observability for ML systems
- Deploy models to WebAssembly, Lambda, and inference servers
- Build real-world projects: transpilers, speech-to-text, and chat applications

---

## GitHub Repositories

### Primary Course Repository

| Repository | Description |
|------------|-------------|
| [paiml/HF-Production-ML](https://github.com/paiml/HF-Production-ML) | Course materials, demos, and specifications |

Clone the repository:

```bash
git clone https://github.com/paiml/HF-Production-ML.git
cd HF-Production-ML
make setup
```

### Sovereign AI Stack (crates.io)

The course uses a pure Rust ML stack. All crates are published on crates.io:

| Crate | Version | Purpose | Link |
|-------|---------|---------|------|
| **trueno** | 0.13.0 | SIMD/GPU compute primitives | [crates.io/crates/trueno](https://crates.io/crates/trueno) |
| **aprender** | 0.24.1 | ML algorithms, APR format | [crates.io/crates/aprender](https://crates.io/crates/aprender) |
| **realizar** | 0.6.8 | Inference server, REST API | [crates.io/crates/realizar](https://crates.io/crates/realizar) |
| **batuta** | 0.5.0 | Orchestration, Pacha registry | [crates.io/crates/batuta](https://crates.io/crates/batuta) |
| **presentar** | 0.3.2 | WASM UI framework | [crates.io/crates/presentar](https://crates.io/crates/presentar) |

### HuggingFace Hub Repositories

| Repository | Purpose |
|------------|---------|
| [paiml/prod-ml-demos](https://huggingface.co/paiml/prod-ml-demos) | Demo models and WASM binaries |
| [paiml/prod-ml-benchmarks](https://huggingface.co/paiml/prod-ml-benchmarks) | Benchmark results and datasets |
| [paiml/depyler-citl](https://huggingface.co/paiml/depyler-citl) | Depyler transpilation models |

### Reference Models (Qwen2.5-Coder Family)

| Model | Parameters | HuggingFace Link |
|-------|------------|------------------|
| Qwen2.5-0.5B | 0.5B (Tiny) | [Qwen/Qwen2.5-0.5B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF) |
| Qwen2.5-Coder-1.5B | 1.5B (Small) | [Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF) |
| Qwen2.5-Coder-7B | 7B (Medium) | [Qwen/Qwen2.5-Coder-7B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF) |
| Qwen2.5-Coder-32B | 32B (Large) | [Qwen/Qwen2.5-Coder-32B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GGUF) |

---

## Course Structure

### Week 1: Model Formats

Understanding ML model formats and the Sovereign AI Stack.

| Lesson | Topics | Videos |
|--------|--------|--------|
| 1.1 | Course Introduction, HuggingFace Publishing | `1.0-course-intro.mp4`, `1.3-hg-model-publish.mp4` |
| 1.2 | Model Types, APR Format Deep Dive | `1.4-model-types-hf.mp4`, `1.5-apr-format.mp4` |
| 1.3 | Format Comparison, Model Tracing | `1.6-mode-format-compare.mp4`, `1.9-why-trace-models.mp4` |

**Key Concepts:**
- GGUF: CPU inference, quantization, consumer hardware
- SafeTensors: Safe loading, HuggingFace standard
- APR: WASM-first format, scales to CUDA
- Pure Rust stack with zero Python runtime

### Week 2: MLOps Foundations

Production infrastructure for ML systems.

| Lesson | Topics | Videos |
|--------|--------|--------|
| 2.1 | Model Registry, CI/CD Pipeline | `2.1-model-registery.mp4`, `2.2-cicd-pipeline-ml.mp4` |
| 2.2 | Observability, Security | `2.3-model-observability-stack.mp4`, `2.4-model-signing-security.mp4` |
| 2.3 | Deployment Patterns, Infrastructure | `2.5-binary-deployment-patterns.mp4`, `2.6-inference-server-architecture.mp4`, `2.8-corpus-management-dataops.mp4`, `2.9-cost-performance-decision-matrix.mp4` |

**Key Concepts:**
- Logs, metrics, and traces for ML observability
- Cryptographic model signing
- Binary deployment patterns (embedded, external, mmap)
- Cost-performance decision matrix

### Week 3: Project Showcase

Real-world projects built with the Sovereign AI Stack.

| Lesson | Topics | Videos |
|--------|--------|--------|
| 3.1 | Depyler Python-to-Rust Transpiler | `3.2-four-projects-one-stack.mp4`, `3.3-depyler-deep-dive.mp4`, `3.4-depyler-oracle.mp4`, `3.5-depyler-single-shot-compile.mp4` |
| 3.2 | Whisper.apr Speech-to-Text | `3.6-whisper-overiew.mp4`, `3.7-whisper-code-walkthrough.mp4`, `3.8-whisper-demo.mp4` |
| 3.3 | APR Ecosystem, Course Conclusion | `3.9-apr-format-rosetta.mp4`, `3.10-apr-hub-spoke.mp4`, `3.11-apr-chat-demo.mp4`, `3.20-course-conclusion.mp4` |

**Key Concepts:**
- Compiler-in-the-loop (CITL) training
- WebAssembly browser inference
- Format conversion: GGUF ↔ SafeTensors ↔ APR
- Hub and spoke architecture for local AI

### Module 4: Capstone Project (Standalone)

Deploy Qwen2.5-Coder-0.5B across all three model formats.

| Component | Description |
|-----------|-------------|
| Part 1 | Model Acquisition & Format Conversion (GGUF → SafeTensors → APR) |
| Part 2 | CLI Deployment (`apr run`, `apr chat`, batch processing) |
| Part 3 | Server Deployment (REST APIs with `apr serve`) |
| Part 4 | Performance Benchmarking (throughput, latency, memory) |
| Part 5 | Format Comparison & Output Equivalence |
| Part 6 | Deployment Scenarios (Edge, Cloud, WASM) |
| Part 7 | Final Report & Analysis |

**Deliverables:**
- Three model files (GGUF, SafeTensors, APR)
- Benchmark JSON files
- Working CLI and server deployments
- Final report with recommendations

**Duration:** 4-6 hours | **Pass Threshold:** 80/100 points

---

## Demo Projects

The course includes 19 hands-on demos across three weeks:

### Week 1: Inference Serving (6 demos)

```bash
make demo-tgi-architecture      # TGI-equivalent concepts
make demo-continuous-batching   # PagedAttention memory simulation
make demo-kv-cache              # KV cache management
make demo-serving-api           # OpenAI-compatible REST API
make demo-streaming             # Server-sent events (SSE)
make demo-throughput-bench      # Performance measurement
```

### Week 2: Model Optimization (7 demos)

```bash
make demo-quantization          # Q4_K, Q5_K, Q6_K methods
make demo-apr-format            # APR native format
make demo-pruning               # Weight pruning
make demo-distillation          # Knowledge distillation
make demo-flash-attention       # Memory-efficient attention
make demo-speculative-decode    # Draft-verify decoding
make demo-tensor-parallel       # Multi-GPU splitting
```

### Week 3: Edge Deployment (6 demos + sub-projects)

```bash
make demo-wasm-inference        # WASM deployment
make demo-lambda-handler        # Serverless patterns
make demo-presentar-ui          # Presentar WASM UI
make demo-latency-profile       # P99 latency measurement
make demo-model-registry        # Pacha push/pull
make demo-hardware-detect       # Runtime capability detection
```

**Sub-Projects:**
- `demos/week3/apr-explorer/` - WASM-based APR format explorer
- `demos/week3/depyler-demo/` - Python-to-Rust transpilation
- `demos/week3/whisper-cli-demo/` - Audio transcription CLI
- `demos/week3/qwen-showcase-demo/` - Interactive Qwen demonstration

---

## HuggingFace Documentation

Essential external documentation for this course:

| Resource | Link |
|----------|------|
| Text Generation Inference | [huggingface.co/docs/text-generation-inference](https://huggingface.co/docs/text-generation-inference) |
| Inference Endpoints | [huggingface.co/docs/inference-endpoints](https://huggingface.co/docs/inference-endpoints) |
| Optimum | [huggingface.co/docs/optimum](https://huggingface.co/docs/optimum) |
| Transformers.js | [huggingface.co/docs/transformers.js](https://huggingface.co/docs/transformers.js) |
| SafeTensors | [huggingface.co/docs/safetensors](https://huggingface.co/docs/safetensors) |

---

## Key Research Papers

The course references foundational ML research:

### Inference & Serving
- Kwon et al. (2023) - [PagedAttention](https://arxiv.org/abs/2309.06180) (SOSP '23)
- Dao et al. (2022) - [FlashAttention](https://arxiv.org/abs/2205.14135) (NeurIPS '22)
- Leviathan et al. (2023) - [Speculative Decoding](https://arxiv.org/abs/2211.17192) (ICML '23)

### Quantization & Compression
- Frantar et al. (2022) - [GPTQ](https://arxiv.org/abs/2210.17323)
- Dettmers et al. (2023) - [QLoRA](https://arxiv.org/abs/2305.14314)
- Lin et al. (2023) - [AWQ](https://arxiv.org/abs/2306.00978)

### Model Architecture
- Qwen Team (2024) - [Qwen2.5-Coder Technical Report](https://arxiv.org/abs/2409.12186)

---

## Live Demos

Interactive demos are deployed at:

| Demo | URL |
|------|-----|
| Course Portal | [interactive.paiml.com](https://interactive.paiml.com) |
| Depyler Transpiler | [interactive.paiml.com/depyler](https://interactive.paiml.com/depyler) |
| APR Explorer | [interactive.paiml.com/apr-explorer](https://interactive.paiml.com/apr-explorer) |

---

## Getting Started

### System Requirements

- **Rust**: 1.83.0 or later
- **OS**: Linux (x86_64) or macOS (ARM64)
- **Optional**: CUDA 12.x for GPU acceleration

### Quick Start

```bash
# Clone the repository
git clone https://github.com/paiml/HF-Production-ML.git
cd HF-Production-ML

# Install dependencies and build
make setup
make build

# Run your first demo
make demo-tgi-architecture

# Run all quality checks
make check
```

### Project Structure

```
HF-Production-ML/
├── docs/
│   ├── outline.md              # Course navigation
│   ├── readings/               # Lesson reading materials
│   ├── scripts/                # Video companion scripts
│   ├── diagrams/               # SVG architecture diagrams
│   └── specifications/         # Full course specification
├── demos/
│   ├── week1/                  # Inference serving demos
│   ├── week2/                  # Optimization demos
│   └── week3/                  # Edge deployment demos
├── Cargo.toml                  # Workspace configuration
└── Makefile                    # Build targets
```

---

## Course Series Overview

This course is part of a 5-course HuggingFace Specialization:

| Course | Title | Focus |
|--------|-------|-------|
| 1 | HF Fundamentals | Transformers library basics |
| 2 | HF NLP | Natural language processing |
| 3 | HF Computer Vision | Image and video models |
| 4 | HF Advanced Fine-Tuning | LoRA, QLoRA, PEFT techniques |
| **5** | **HF Production ML** | **Deployment and MLOps** |

**Total Specialization:** 15 weeks, ~60 hours

### Course 5 Module Breakdown

| Module | Focus | Duration |
|--------|-------|----------|
| Week 1 | Model Formats (GGUF, SafeTensors, APR) | ~3 hours |
| Week 2 | MLOps Foundations (CI/CD, Observability) | ~4 hours |
| Week 3 | Project Showcase (Depyler, Whisper, APR) | ~3 hours |
| **Capstone** | **Multi-Format Deployment Project** | **4-6 hours** |

**Course 5 Total:** 10 lessons, 4 modules, ~14-16 hours

---

## Support and Community

- **Issues**: [github.com/paiml/HF-Production-ML/issues](https://github.com/paiml/HF-Production-ML/issues)
- **Discussions**: [github.com/paiml/HF-Production-ML/discussions](https://github.com/paiml/HF-Production-ML/discussions)

---

*Last updated: January 2026*
