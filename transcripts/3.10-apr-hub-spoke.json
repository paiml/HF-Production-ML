{"text":" Here we have the Quinn Coder Showcase, which is one of the ways that the APR tooling has been demonstrated to be successful. In terms of the ecosystem, the three places that are the easiest to start are APR chat, which is the chat system, APR run, which is the ability to do a single shot inference and then APR server for doing OpenAI compatible endpoints. Now this works with all of the model formats like and Safe Tensors, and you can download a local and then have your own local sovereignty I stack. So this is really the innovation is that instead of thinking about using other people's tools or other people's external APIs, can you turn off the internet? Can you have this all running local? And can you have world class CPU performance because of assembly level optimizations with SIMD? You can also GPU performance and can you find way. This is the ecosystem in a nutshell. If we look at the three modes here and go into them a little bit more, you have the ability to chat through interactive repel. This was a system prompt to adjust the temperature, et cetera. And for the run, you have one prompt in, one response out. And then for a serve, you have a rest API. Now it also supports the ability to batch. So you can get really good world class performance by batching then in terms of the format. The three formats that are supported are GGUF, which is the Lama CPP format. We have the API format, which is the native format. And the reason why this format was created is that because this is rust, we want the ability to bundle these models together into a binary so you can deploy it in WebAssembly, where you can actually deploy it to an arm-based executable in terms of safety. where we have the hugging face format as well. This is a standard, so we support it. You can do chat, run, et cetera, but also you can convert it to the APR format. In terms of the model tiers here as well, we're focusing on the Quinn for this particular example because it's a really good example of a modern open source model. It has tiny small medium and large, so it's a good way to exercise the imports, the serving, the inference, in terms of making sure that this is reliable ecosystem. So the APR-based ecosystem is an emerging standard for doing things with model conversions, with model inference, and the key takeaway is that the reason even exists is this ability to bundle the model into a binary and run it on both world-class GPUs as well as tiny web assembly targets.","segments":[]}