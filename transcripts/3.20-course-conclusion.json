{"text":" All right, course is complete. You made it. Let's talk through what we covered in week one. We covered the model formats. You learned about GGUF for quantized models, safe tensors for safe loading, and then APR for the sovereign stack. In week two, we got into MLOPS foundations, CI/CD pipelines actually work observability with logs, metrics, and traces. We also got into security with model signing in week three. We got into the type of can build via the project showcase. So, Deployer, Transpile, Python to Rust and also compile Python to Rust, Whisper for Speech to Text and also using Quinn inside of the APR command ecosystem. We also talked as well about how you can use APR shell to do auto completion, run it in the browser, or even run it local to make your own type of Z shell automation. In terms of the take to remember if we're diving into the details here are the sovereign stack itself. So pure rust you don't need Python for LLM ops and you can build the entire pipeline local for maximum performance and minimum dependencies. Also about the different kinds of formats GGUF, safe tensor, APR. You can convert between these different formats. If you understand the trade-offs, we'll also talk about MLops. observability dashboards also signed models and then the deploy anywhere. The idea here with the deploy anywhere is that when you build for GPU for speed, CPU for availability and web assembly for browser and edge for IoT, you can have one stack in all of the targets. This is definitely the kind of thinking that's necessary to get us to the next level when we're thinking about using models in production. So what's next? The idea is here you to fork the demos, the code is yours, right? It's open source, modify it, break it, learn from it, build your own models, use the stack, use APR to run and serve. You can use Intranon or to train things. You can also deploy to production, pick a target, ship the code. The browser is often a great target for web assembly. Also, command line tools are a great target because you can bundle the model into the binary. So in a nutshell, ship it, that's the goal, production, ML, real system. real users. All right, talk to you soon.","segments":[]}