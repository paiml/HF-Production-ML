{"text":" The Infraint Server Architecture shows how APR-Serve will handle requests at scale. Let's take a look at the request flow first. The request follow in terms of the client where you could have multiple concurrent requests hitting a server and then you have a low-bouncer and the low-bouncer would have health checks, round robin, queue management, and you distribute the load evenly. In terms of a queue, this is where you could have a of GPU memory, this is where the compute occurs. So you could have single or multiple GPUs and then you could have a key value cache with page attention blocks. This allows you to serve many concurrent requests and then in terms of what comes back, it would be stream tokens and it would be open AI compatible format. Now in terms of a GPU pool here, the next thing to think about is this is where the inference is actually a that are taking advantage of things like Kuda and you could have multiple GPUs working in parallel and you could see how memory utilization could show the load. So this is a place to consider when you're serving is that in terms of real-time GPUs, it's going to give you very good performance. If you care about energy efficiency and you don't care about the ultimate one token speed, then CPU is well because you could do batch based inference and the batch based inference could be very cost effective. In terms of API endpoints, this is where we would do things like chat completions for chat inference. So slash v1 slash chat slash completions or v1 completions. This could be text completion. You could have slash health for readiness and you could have slash metrics and this would be things like feeding your dashboard in terms of the streaming responses come in server-cent events and then the token streams as they're generated. So users would see the output immediately and you get better UX for waiting in terms of the completion. So in a nutshell, it's important to think about inference servers in largely with models, the load mounts or the queue, the batch schedule, the GPU pool, and it's going to depend on the kind of problems or solving. GPU is going to give you very good response time and has the ability to leave things in memory, but if you care more about cost and you don't care about ultimate speed, then CPU, especially if it's SIMD optimized, can be a very effective place for inference as well.","segments":[]}