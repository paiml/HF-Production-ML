{"text":" The model registry architecture for hugging face has a hub that organizes everything. In the center here, we have the hub where the model lives in the case of a URI. It could be hugging face, org, model name, and you have one address in all the artifacts live there. So it could even have multiple models in terms of the types like a GGUF and a safe tensor or an APR. And then in terms of the art four artifact types in every repository. There's the model weights. So it could be safe tensors, GGUF, or the APR format. These are the actual parameters. There's also the configuration. So the config.json tells the layer count, the hidden dimensions, the vocabulary size, the tokenizer.json, et cetera. In the case again of the single purpose models, they don't have those configurations, but in the case of safe tensors we do. LFS. This is for files over the 10 megabyte automatic limit and you would put the large file storage in and you can have up to 50 gigabytes per file, meaning that in the case of a very large file you would shard it. You also have the model card, so read me at license and this is where the documentation would live with the model. You also have the URI resolution. So for example, if you do APR pull or Olamapull, it's going to be repo and the file and then you're going to download it via LFS and then you're going to cache locally. So in the case of the APR ecosystem, you'll go to tilde slash dot cache, patch a models in a llama, same type of technique or some other tool. You pull it into the cache and then you're able to run it local. In terms of versioning, we also have get based versioning so every changes are commit and you have the ability to roll back anytime. So in a nice registry and this registry holds all of the artifacts and you cache and control that locally and then push back when you have to make changes.","segments":[]}