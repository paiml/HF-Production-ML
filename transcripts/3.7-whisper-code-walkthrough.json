{"text":" Here we have Whisper.pl repo. This contains all of the code to do both CLI based inference with the Whisper model as well as the ability to run a web assembly application. Let's take a look at the design of this project. You can see there's a lot of code for a real world large language model application that does something with the model. If we look at this structure here, we can see audio, back end, CUDA, we have detection, we also have things like the tokenizer, we also have the vocabulary. So these are core components that have to be introduced into the model. If we take a look here at the web assembly as well, you can see here that there are a few things to point out. For example, at the very top here on the mod, you can see that the web assembly bindings provide JavaScript friendly API, but it's done in pure Russ. some workers here as well that allow us to do a message-based communication between the main thread and the worker thread. And this is critical because when you're building something that's this complex, for example, a browser-based inference that runs with the CPU, which is in fact the future of large language models is the ability to run these little models inside of a web browser. There is a architectural design that's important. ability to have things running in a main thread and also a worker thread because the main thread has to respond to the user and the work has to be done in the background and that's what this worker thread does. Now if we take a look here as well at the CLI, this is a very important thing to point out is that inside of the CLI, it has the same functionality as the web assembly except for its design for doing you can see here that we have a structure of mod.rs. We have args.rs and command.rs and output. So in the case of the output, what do we want the transcription to do? Is it a text or maybe we want some timestamps with it or a JSON, depending on what it is that we're doing. And then if you go down here and you look at some of these commands, you'll be able to see this command structure shows the actual arguments that go in. or in the case of timing or the result or failure. All of this is wired into the beginning here. So we have transcribed, translate, summarize, stream, serve. And what's powerful about this is that even though there's a lot of rust code at the core, what is the thing that's doing the work? It's the model. And so this is the structure that will be more and more prevalent as people that are 20 megabytes or two megabytes is this ability to wrap around automation to these tools be able to run them on not just GPU but CPU with acceleration like SIMD and also to solve real world problems without having this send your data to an external vendor. So in this case I would say just clone this repo. You can see that it's on the pragmatic AI labs website fee. You can say get clone P-A-I-M-L slash whisper.fr. Try it out, fork it, make your own automation with it, or you can just car go install it, but this is a good template for the kinds of things you can do with large language models that run local and that run on the CPU.","segments":[]}