{"text":" The APR format ecosystem has a universal model format conversion, GGUF, safe tensors, and APR. If we take a look at the Rosetta Stone module here, we can look at the fact that it translates different model types between each other. These three formats have six conversion paths. So all of them are bidirectional. We have GGUF, which is the Lama.cpp format. This is good for default. tensors format is something that's going to be very commonly experienced in a wild. This is hugging faces format. It also is a emergent fix from the pickling issues of the earlier ecosystem. We also have APR which is a new emerging format that has many of the best practices for both but also the ability to bundle into a rest binary so that you can package the model in the binary If we take a look at this format conversion here, a few things to point out is that you want to make sure that you're respecting the quantization in the case of a PR, it supports the same levels of quantization, and then also with safe tensor. So if you're converting from one model to another and then you're polluting or you're corrupting the actual model format, then that's a problem. Also, we have to figure out the alignment, for example, APR it uses 64 byte for better M map and safe tensors This is not something that we natively look at when we're doing a conversion in terms of the compression We look at also how to compress to get the smallest model because a lot of times the APR model will run in the browser in terms of signatures APR also has the ability to sign the models in terms of the types of models that are It's any model that's supported. There's a lot of support initially for Quinn. And if we look at the import pipeline, this is an important component is that you can import from hugging face directly. So you parse the URI downloaded as well, validated, go through a checklist. And then if you need to convert it to the APR format, so you could bundle it inside of a Rust binary. And then maybe deploy it inside of a format has a lot of promise for doing small embedded based deployments, either to a arm-based device like Raspberry Pi or some kind of embedded device as well as also running inside of a web browser or if you need to embed the model directly into a rest binary.","segments":[]}