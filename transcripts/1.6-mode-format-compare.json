{"text":" A complete ML model needs four components. The question is how do you package them together? Different model formats have different ideas on how to do this. We can think about weights, we can think about tokenizers, config, and metadata. Let's take a look at these three formats specifically. First up, we have the hugging phase standard, which is safe tensors. If you look at the safe tensors format, it has four separate files. we have the tokenizer, we also have a config and then the metadata is somewhere else. So the idea here is that on one hand it makes it really simple as a developer to know what layer you're working at inside of the model. You can go in, you can look at a piece and say, okay, here's the thing that I need to change, but also there are some problems because it makes it easy to have a version mismatch. So for example, what if you deploy a have a version one of tokenizer and you have a silent corruption. There's some risks to this format as well as benefits to this format. It's good to know and actually be able to make the trade offs. In terms of GGUF, this is also an emerging standard you see this with Lama CPP, also with Olamma, which is a very popular runner and GGUF put it all together. So there's one file, everything's inside. They're bundled. There's no mismatch one issue is that how do you know if the model is corrupt or not corrupted? And that's something that is also starting to become an emerging standard is do we have a checksum of this particular model so we can confirm it has everything that we need. Another emerging format is the AR format. APR takes a little bit further. You have the same file, everything's bundled. But you also can do cryptographic checksums. corruption, you can also have provenance, like where did this data come from, how do I know someone trained it ethically? It also has the concept of self-contained and verified. So we can see that there are lots of different ideas for model formats, we'll see lots of different conversions from one format to another, and it's something that's really important in terms of understanding how to deploy models is to look at the particular formats themselves know the tradeoffs.","segments":[]}