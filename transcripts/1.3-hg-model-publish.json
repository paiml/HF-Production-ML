{"text":" Before we dive into inference serving in optimization, let's understand the basics. What goes into hugging face model repositories themselves? It's a little bit like a recipe box, so you need the instructions, you need the ingredient list, and also the dish itself. If we look at the publishing flow here, it's going to start on a local machine or a machine that cloud and you train or find human model and you already have the have the model weights, you have a config file and a read me. If you look at using a command line tool, you can publish it that way or if you use the Rust sovereignty I stack that's APR publish and then you can use a standard CLI tool as well to double check that that upload occurred with a upload via Rust or CLI tool to the hub, then that model is going to get its own page. It's going to get a URL and that's going to be Once it's on the hub then anybody can download it. You can fine tune it. You can even deploy it to production as well. Depending on what the type is and what it is you're trying to do with it. That's really the power of open model sharing and especially as we get into making things simpler and more binary focused we're going to see more and more optimizations with these smaller models. In terms of the model folder here, this is the model card. be something that allows you to look at things and look at the config digiase and the model weights, oftentimes it's going to be the safe tension format or the emerging.apr format. The rest will be optional. In terms of the config digiase and this is the blueprint, it's going to tell you how your code can reconstruct the neural network, how many layers, hit dimensions. This is something that's going to make things meful versus just random numbers and a file. safe tinsers, this would be where all the parameters are. And they're safe because they're avoiding that old pickle based PyTorch model format. And then in terms of the tokenizer.json, this is where you translate. And so the tokenizer converts, let's say, hello world into tokens. So again, with the tokenizer, it's going to be using a pure rust subword tokenizer, which has very high performance and very good accuracy. So a few things to consider, that files over 10 megabytes will automatically use the get LFS or the large files storage. And then the maximum of 50 gigs per file is going to allow you to have large models, but they would be shorted. And public reposts have unlimited storage. So the idea here is that if you're using the Russ sovereignty iStack, you would do APR publish and you just pass in the model and then you pass in a license and tags. the hugging face CLI. You first authenticate and then upload and then it's going to be similar with some other steps. So in a nutshell, the model format is a lot like what you would be doing if you're publishing code to let's say GitHub or get lab or something like that. Except for in this case, it's going to be models and the same technique applies. I think what we're seeing though is that beyond just the scripting language type interfaces for building with models, we're going to see more and more pure binary-based tools that make it much simpler to interact fine-tune and also get incredible performance and safety when you push these into production.","segments":[]}