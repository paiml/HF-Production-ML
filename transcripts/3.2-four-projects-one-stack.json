{"text":" A good example of what happens when you have a sovereignty I stack is you have the ability to use lots of different solutions at the same time, build on the core foundations. In this example here we see there's four projects that I want to talk about. We have Deplier in the idea with Deplier is you take Python, you transpile it to Rust, and then you compile it. What we've seen in developing this project is that to build custom Oracle's that are trained on the success and failure rates so that it makes it easier to train on future corpus. For example, a company may be adding new libraries or changing and they want to constantly be converting from Python to Rust. They can use this Oracle that's built inside. The Oracle itself uses the APR model format, but beneath the hood, it also uses Trinion. I've seen in testing that we have of 20 or even more examples all the way up to 200 examples based on a variety of different examples of Python from CLI tools to data science where we can get approaching 80% single shot compile. In terms of the next project, APR shell, this is a good example of a very tiny model that runs in the browser, but also runs local. well that I personally use every single day and this APR shell allows us to easily customize and train things because I have a model format and this is a good example of what we'll likely see in the future as people use open source models, share open source models is instead of only sharing code if you have a model let's say you know a megabyte or 500k that model itself may come bundled with you allow people to customize it. And in fact, I have customized it to use my own solutions around automating ZSH. A third example is whispered IDPR. This shows a lot of the importing from hugging face. And so if you think of hugging face as a repository for code, that has to do with models or data that has to do with models, you can see that with whispered IDPR is able to pull them face. Use all of the weights, all of the vocabulary, the embeddings, put those into a single model. In this case, the whispered out API model and get extremely fast performance and allows you as well to run it in a web browser and also to do batch-based transcriptions. So what's nice about this is it's a strong proof of concept for how you can actually pull and change and manipulate things locally using one of the premier examples of some of the things you can do when you have your own sovereign stack that can talk to HuggingFace is the ability to pull models down and then change them. For example, let's say you want to change them from safe tensor format to APR format or safe tensor format to GGUF format. I want to quantize them or shrink them or do some kind of modification or prune them. These are all things that we can do. to change let's say the chat template or the serve template where I want to add some automation to it. These are all things that we show in the Quen showcase example in operandere repo. So what's nice about this is that you can see that there are distinct libraries, training for assembly level accelerated computing for both SIMD and GPU. We also have a prandere which is the center place for doing deep learning and in statistics and also has the.apr format. And then in terms of inference, this is where Realizar comes in and it also has the ability to do CPU based inference, GPU based inference, and it's competitive with the best inference servers in the world. And in some cases, we see many examples of how it could excel beyond that because it's built on first principles and rust. So thinking about things in terms of a sovereign AI stack future of large language models, small language models, and machine learning, and it's something that we are heavily invested in at Pragmatic AI Labs.","segments":[]}